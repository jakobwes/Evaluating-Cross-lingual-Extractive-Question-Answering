{"mlqa": {"layers 1": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 3.561387066541706, "f1": 9.287217406559414, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 2.9013539651837523, "f1": 8.771017266573653}, "predict results": {"exact_match": 3.561387066541706, "f1": 9.287217406559414, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 3.8425492033739457, "f1": 9.358136400942845, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 2.9013539651837523, "f1": 8.085925540581128}, "predict results": {"exact_match": 3.8425492033739457, "f1": 9.358136400942845, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 4.272747398715962, "f1": 10.240902689345864, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 3.7109375, "f1": 9.949156937552228}, "predict results": {"exact_match": 4.272747398715962, "f1": 10.240902689345864, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 3.6925960637300843, "f1": 9.391672633659129, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 2.9013539651837523, "f1": 9.028978915654047}, "predict results": {"exact_match": 3.6925960637300843, "f1": 9.391672633659129, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 3.9849457604604828, "f1": 9.798959112701475, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 3.125, "f1": 9.992549726823826}, "predict results": {"exact_match": 3.9849457604604828, "f1": 9.798959112701475, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 3.5030198446937013, "f1": 9.1798780577241, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 3.1358885017421603, "f1": 9.16210309153379}, "predict results": {"exact_match": 3.5030198446937013, "f1": 9.1798780577241, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 3.959642109270893, "f1": 11.498574890706148, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 6.0, "f1": 13.618349269516724}, "predict results": {"exact_match": 3.959642109270893, "f1": 11.498574890706148, "predict_samples": 5457}}}, "layers 2": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 3.6738519212746015, "f1": 10.70891490327402, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 2.7079303675048356, "f1": 9.485031758525691}, "predict results": {"exact_match": 3.6738519212746015, "f1": 10.70891490327402, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 3.5238987816307406, "f1": 10.66934412497829, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 2.514506769825919, "f1": 10.183488521109485}, "predict results": {"exact_match": 3.5238987816307406, "f1": 10.66934412497829, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 4.051361523134824, "f1": 11.13917063077984, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 3.90625, "f1": 10.902849474481267}, "predict results": {"exact_match": 4.051361523134824, "f1": 11.13917063077984, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 3.880037488284911, "f1": 10.105475889633395, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 3.6750483558994196, "f1": 9.866951407166676}, "predict results": {"exact_match": 3.880037488284911, "f1": 10.105475889633395, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 4.361301748948417, "f1": 10.796566446615108, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 4.4921875, "f1": 11.978754487315594}, "predict results": {"exact_match": 4.361301748948417, "f1": 10.796566446615108, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 4.115616911130285, "f1": 9.97259268548838, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 4.70383275261324, "f1": 10.826207845451064}, "predict results": {"exact_match": 4.115616911130285, "f1": 9.97259268548838, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 4.264229963830192, "f1": 12.461148782784784, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 5.4, "f1": 14.370697026046198}, "predict results": {"exact_match": 4.264229963830192, "f1": 12.461148782784784, "predict_samples": 5457}}}, "layers 3": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 3.580131208997188, "f1": 10.563566046469557, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 2.514506769825919, "f1": 9.842734120085067}, "predict results": {"exact_match": 3.580131208997188, "f1": 10.563566046469557, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 3.486410496719775, "f1": 10.577372696022225, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 2.9013539651837523, "f1": 10.493759716444231}, "predict results": {"exact_match": 3.486410496719775, "f1": 10.577372696022225, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 5.1361523134824, "f1": 12.411389212156436, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 5.6640625, "f1": 12.331145365779026}, "predict results": {"exact_match": 5.1361523134824, "f1": 12.411389212156436, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 5.042174320524836, "f1": 11.098947157898397, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 5.029013539651838, "f1": 11.279975746759751}, "predict results": {"exact_match": 5.042174320524836, "f1": 11.098947157898397, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 5.423953951737879, "f1": 11.85622978727067, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 5.859375, "f1": 12.628200581732518}, "predict results": {"exact_match": 5.423953951737879, "f1": 11.85622978727067, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 5.176876617773943, "f1": 11.020615391469054, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 4.965156794425087, "f1": 11.13767691503219}, "predict results": {"exact_match": 5.176876617773943, "f1": 11.020615391469054, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 5.4064344184275654, "f1": 13.496777160530806, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 7.4, "f1": 15.154501489203792}, "predict results": {"exact_match": 5.4064344184275654, "f1": 13.496777160530806, "predict_samples": 5457}}}, "layers 4": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 4.423617619493908, "f1": 12.246014689263692, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 3.481624758220503, "f1": 12.487764610525373}, "predict results": {"exact_match": 4.423617619493908, "f1": 12.246014689263692, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 3.7300843486410495, "f1": 10.90518265247438, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 3.288201160541586, "f1": 11.960245742228313}, "predict results": {"exact_match": 3.7300843486410495, "f1": 10.90518265247438, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 6.641576267434138, "f1": 14.087846569271452, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 6.25, "f1": 13.273758196656555}, "predict results": {"exact_match": 6.641576267434138, "f1": 14.087846569271452, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 5.454545454545454, "f1": 11.27254874685508, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 6.769825918762089, "f1": 12.23040684434985}, "predict results": {"exact_match": 5.454545454545454, "f1": 11.27254874685508, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 6.553021917201683, "f1": 13.144650225325984, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 6.0546875, "f1": 13.517363055705909}, "predict results": {"exact_match": 6.553021917201683, "f1": 13.144650225325984, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 6.764452113891285, "f1": 12.833485841941554, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 6.445993031358885, "f1": 12.940092771733857}, "predict results": {"exact_match": 6.764452113891285, "f1": 12.833485841941554, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 6.796116504854369, "f1": 14.962953362382095, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 8.6, "f1": 17.06577691257792}, "predict results": {"exact_match": 6.796116504854369, "f1": 14.962953362382095, "predict_samples": 5457}}}, "layers 5": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 5.698219306466729, "f1": 12.290600593617405, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 4.6421663442940035, "f1": 11.633161286924736}, "predict results": {"exact_match": 5.698219306466729, "f1": 12.290600593617405, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 5.098406747891284, "f1": 11.499135965837235, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 3.8684719535783367, "f1": 10.073669204877286}, "predict results": {"exact_match": 5.098406747891284, "f1": 11.499135965837235, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 7.504981182200575, "f1": 14.60974447642356, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 6.640625, "f1": 13.067350610433303}, "predict results": {"exact_match": 7.504981182200575, "f1": 14.60974447642356, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 6.260543580131209, "f1": 11.657985534851642, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 6.963249516441006, "f1": 11.901112294617032}, "predict results": {"exact_match": 6.260543580131209, "f1": 11.657985534851642, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 7.195040956386983, "f1": 13.248269465210237, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 5.859375, "f1": 11.518274169967672}, "predict results": {"exact_match": 7.195040956386983, "f1": 13.248269465210237, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 7.057808455565143, "f1": 12.782478571383924, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 6.7073170731707314, "f1": 12.003070775459777}, "predict results": {"exact_match": 7.057808455565143, "f1": 12.782478571383924, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 7.19588806396345, "f1": 14.972621721243843, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 7.8, "f1": 16.612430045055216}, "predict results": {"exact_match": 7.19588806396345, "f1": 14.972621721243843, "predict_samples": 5457}}}, "layers 6": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 6.091846298031865, "f1": 13.088969556692893, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 6.382978723404255, "f1": 12.774344307670136}, "predict results": {"exact_match": 6.091846298031865, "f1": 13.088969556692893, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 5.192127460168697, "f1": 11.57386718573, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 5.996131528046422, "f1": 11.76860802305287}, "predict results": {"exact_match": 5.192127460168697, "f1": 11.57386718573, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 8.656187735222494, "f1": 16.049855966016235, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 8.0078125, "f1": 14.613310035400126}, "predict results": {"exact_match": 8.656187735222494, "f1": 16.049855966016235, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 6.672914714151828, "f1": 11.841552402154894, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 7.7369439071566735, "f1": 12.738193301184458}, "predict results": {"exact_match": 6.672914714151828, "f1": 11.841552402154894, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 8.191277396502104, "f1": 13.980900056949379, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 7.6171875, "f1": 13.259837085165055}, "predict results": {"exact_match": 8.191277396502104, "f1": 13.980900056949379, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 8.912855910267472, "f1": 14.60891112634602, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 9.40766550522648, "f1": 14.899974121447586}, "predict results": {"exact_match": 8.912855910267472, "f1": 14.60891112634602, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 8.928231486769466, "f1": 17.11346538137338, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 9.2, "f1": 16.83033082403457}, "predict results": {"exact_match": 8.928231486769466, "f1": 17.11346538137338, "predict_samples": 5457}}}, "layers 7": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 6.279287722586692, "f1": 12.66289892456204, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 5.996131528046422, "f1": 12.912343698044726}, "predict results": {"exact_match": 6.279287722586692, "f1": 12.66289892456204, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 5.473289597000937, "f1": 11.62712905235625, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 5.222437137330754, "f1": 11.150032241927091}, "predict results": {"exact_match": 5.473289597000937, "f1": 11.62712905235625, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 8.346247509408899, "f1": 14.877836863147975, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 8.984375, "f1": 16.204499667280263}, "predict results": {"exact_match": 8.346247509408899, "f1": 14.877836863147975, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 6.41049671977507, "f1": 11.475693931315815, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 7.93036750483559, "f1": 12.215851077014253}, "predict results": {"exact_match": 6.41049671977507, "f1": 11.475693931315815, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 8.301970334292673, "f1": 13.85947089357417, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 8.203125, "f1": 13.581116196254964}, "predict results": {"exact_match": 8.301970334292673, "f1": 13.85947089357417, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 8.343399482312337, "f1": 13.774582421316515, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 8.710801393728223, "f1": 13.520638007672725}, "predict results": {"exact_match": 8.343399482312337, "f1": 13.774582421316515, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 8.338092518560822, "f1": 16.297011051681686, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 8.2, "f1": 16.09142546524256}, "predict results": {"exact_match": 8.338092518560822, "f1": 16.297011051681686, "predict_samples": 5457}}}, "layers 10": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 5.885660731021556, "f1": 12.69565261946632, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 5.222437137330754, "f1": 12.11516862640829}, "predict results": {"exact_match": 5.885660731021556, "f1": 12.69565261946632, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 4.517338331771321, "f1": 10.807988428500193, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 4.448742746615087, "f1": 11.404355654742002}, "predict results": {"exact_match": 4.517338331771321, "f1": 10.807988428500193, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 8.368386096967013, "f1": 15.719180678641981, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 6.4453125, "f1": 14.021541374191301}, "predict results": {"exact_match": 8.368386096967013, "f1": 15.719180678641981, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 6.016869728209935, "f1": 11.43416972126703, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 6.189555125725338, "f1": 12.656174594848657}, "predict results": {"exact_match": 6.016869728209935, "f1": 11.43416972126703, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 7.792782820456055, "f1": 14.195729353554366, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 7.03125, "f1": 13.478125943935455}, "predict results": {"exact_match": 7.792782820456055, "f1": 14.195729353554366, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 8.645383951682485, "f1": 14.744197185540017, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 8.710801393728223, "f1": 15.540718645615028}, "predict results": {"exact_match": 8.645383951682485, "f1": 14.744197185540017, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 8.623643632210166, "f1": 16.581597776591423, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 7.0, "f1": 16.01103084656797}, "predict results": {"exact_match": 8.623643632210166, "f1": 16.581597776591423, "predict_samples": 5457}}}, "layers 11": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 5.49203373945642, "f1": 12.431131297404717, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 6.189555125725338, "f1": 13.221257950938668}, "predict results": {"exact_match": 5.49203373945642, "f1": 12.431131297404717, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 4.029990627928772, "f1": 10.549690618994942, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 4.25531914893617, "f1": 11.34718942431568}, "predict results": {"exact_match": 4.029990627928772, "f1": 10.549690618994942, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 7.372149656851893, "f1": 14.656676422540098, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 6.25, "f1": 13.816136484962879}, "predict results": {"exact_match": 7.372149656851893, "f1": 14.656676422540098, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 6.204311152764761, "f1": 11.477660744266014, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 6.769825918762089, "f1": 12.605538484849943}, "predict results": {"exact_match": 6.204311152764761, "f1": 11.477660744266014, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 7.394288244410006, "f1": 13.90679779282692, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 6.640625, "f1": 12.143147712161701}, "predict results": {"exact_match": 7.394288244410006, "f1": 13.90679779282692, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 8.12769628990509, "f1": 14.196073440157289, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 8.362369337979095, "f1": 14.039673712859177}, "predict results": {"exact_match": 8.12769628990509, "f1": 14.196073440157289, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 7.709880068532267, "f1": 15.901409941157553, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 7.2, "f1": 16.152769590728475}, "predict results": {"exact_match": 7.709880068532267, "f1": 15.901409941157553, "predict_samples": 5457}}}, "layers 12": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 3.8612933458294285, "f1": 11.325533752795476, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 3.094777562862669, "f1": 10.546934909009675}, "predict results": {"exact_match": 3.8612933458294285, "f1": 11.325533752795476, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 3.486410496719775, "f1": 10.61424147578822, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 2.514506769825919, "f1": 9.599722145507279}, "predict results": {"exact_match": 3.486410496719775, "f1": 10.61424147578822, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 3.8742528226699138, "f1": 12.137370865546805, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 3.90625, "f1": 12.15532723678246}, "predict results": {"exact_match": 3.8742528226699138, "f1": 12.137370865546805, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 4.048734770384255, "f1": 10.225017557867684, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 5.802707930367505, "f1": 11.751016303297014}, "predict results": {"exact_match": 4.048734770384255, "f1": 10.225017557867684, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 4.826212087668806, "f1": 11.87154221769941, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 4.4921875, "f1": 13.169041320608207}, "predict results": {"exact_match": 4.826212087668806, "f1": 11.87154221769941, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 4.391716997411562, "f1": 10.986734042168901, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 5.2264808362369335, "f1": 11.601019592346109}, "predict results": {"exact_match": 4.391716997411562, "f1": 10.986734042168901, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 5.2922139729678275, "f1": 14.246399096086462, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 5.0, "f1": 14.246378650897961}, "predict results": {"exact_match": 5.2922139729678275, "f1": 14.246399096086462, "predict_samples": 5457}}}, "layers 8": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 6.785379568884723, "f1": 13.984416791205675, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 5.802707930367505, "f1": 12.898541639767215}, "predict results": {"exact_match": 6.785379568884723, "f1": 13.984416791205675, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 5.28584817244611, "f1": 11.5914591909561, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 5.029013539651838, "f1": 11.294062135298695}, "predict results": {"exact_match": 5.28584817244611, "f1": 11.5914591909561, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 9.829532875802524, "f1": 17.378852793285503, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 8.984375, "f1": 15.847497609721566}, "predict results": {"exact_match": 9.829532875802524, "f1": 17.378852793285503, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 6.954076850984068, "f1": 12.223942353542451, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 8.704061895551257, "f1": 14.249242933060579}, "predict results": {"exact_match": 6.954076850984068, "f1": 12.223942353542451, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 9.253929599291565, "f1": 15.31719450032373, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 8.3984375, "f1": 14.655399811199219}, "predict results": {"exact_match": 9.253929599291565, "f1": 15.31719450032373, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 10.310612597066436, "f1": 16.55063111816083, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 10.714285714285714, "f1": 16.859735801102996}, "predict results": {"exact_match": 10.310612597066436, "f1": 16.55063111816083, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 10.184656386826575, "f1": 18.5935296638866, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 9.4, "f1": 17.358608235519952}, "predict results": {"exact_match": 10.184656386826575, "f1": 18.5935296638866, "predict_samples": 5457}}}, "layers 9": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 6.597938144329897, "f1": 13.271721699814012, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 5.802707930367505, "f1": 12.394076020875875}, "predict results": {"exact_match": 6.597938144329897, "f1": 13.271721699814012, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 5.323336457357076, "f1": 11.663220803618088, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 5.029013539651838, "f1": 10.958075912588454}, "predict results": {"exact_match": 5.323336457357076, "f1": 11.663220803618088, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 8.479079034757582, "f1": 15.390446019783868, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 7.03125, "f1": 14.227832437287466}, "predict results": {"exact_match": 8.479079034757582, "f1": 15.390446019783868, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 6.69165885660731, "f1": 11.896584162669098, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 8.897485493230175, "f1": 14.052158389218476}, "predict results": {"exact_match": 6.69165885660731, "f1": 11.896584162669098, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 8.191277396502104, "f1": 14.129945921858646, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 7.8125, "f1": 13.987118809338819}, "predict results": {"exact_match": 8.191277396502104, "f1": 14.129945921858646, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 8.999137187230371, "f1": 14.752676842521499, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 8.101045296167248, "f1": 13.964453273768816}, "predict results": {"exact_match": 8.999137187230371, "f1": 14.752676842521499, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 8.566533409480297, "f1": 16.612964843859647, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 7.4, "f1": 16.209065029024174}, "predict results": {"exact_match": 8.566533409480297, "f1": 16.612964843859647, "predict_samples": 5457}}}}}