{"mlqa": {"layers 1": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 2.9615745079662608, "f1": 8.672596094894304, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 2.9013539651837523, "f1": 9.39556072933021}, "predict results": {"exact_match": 2.9615745079662608, "f1": 8.672596094894304, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 3.486410496719775, "f1": 9.081305262487849, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 3.094777562862669, "f1": 8.701954127471069}, "predict results": {"exact_match": 3.486410496719775, "f1": 9.081305262487849, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 4.272747398715962, "f1": 10.125323463997436, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 3.7109375, "f1": 10.406414831027499}, "predict results": {"exact_match": 4.272747398715962, "f1": 10.125323463997436, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 4.0674789128397375, "f1": 9.819273866794042, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 3.8684719535783367, "f1": 9.749116859636258}, "predict results": {"exact_match": 4.0674789128397375, "f1": 9.819273866794042, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 3.9849457604604828, "f1": 9.936149692880212, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 3.3203125, "f1": 9.935055205797322}, "predict results": {"exact_match": 3.9849457604604828, "f1": 9.936149692880212, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 3.62381363244176, "f1": 9.163865619922458, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 3.397212543554007, "f1": 9.138452953845187}, "predict results": {"exact_match": 3.62381363244176, "f1": 9.163865619922458, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 4.092899295640587, "f1": 11.703373402075592, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 4.8, "f1": 13.670771545472322}, "predict results": {"exact_match": 4.092899295640587, "f1": 11.703373402075592, "predict_samples": 5457}}}, "layers 2": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 3.1490159325210874, "f1": 10.236659134185087, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 2.514506769825919, "f1": 9.490870914005828}, "predict results": {"exact_match": 3.1490159325210874, "f1": 10.236659134185087, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 2.867853795688847, "f1": 9.752805682685292, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 2.9013539651837523, "f1": 10.23784092833417}, "predict results": {"exact_match": 2.867853795688847, "f1": 9.752805682685292, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 4.294885986274076, "f1": 11.471084797514678, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 5.6640625, "f1": 13.181343095967238}, "predict results": {"exact_match": 4.294885986274076, "f1": 11.471084797514678, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 4.423617619493908, "f1": 10.670715703387073, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 5.029013539651838, "f1": 10.514399098868964}, "predict results": {"exact_match": 4.423617619493908, "f1": 10.670715703387073, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 4.649103387203897, "f1": 10.878842693838155, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 4.8828125, "f1": 12.050796199553153}, "predict results": {"exact_match": 4.649103387203897, "f1": 10.878842693838155, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 4.305435720448663, "f1": 10.361270320395192, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 5.2264808362369335, "f1": 11.252172959412098}, "predict results": {"exact_match": 4.305435720448663, "f1": 10.361270320395192, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 4.53074433656958, "f1": 12.78303969481134, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 7.2, "f1": 16.367154883267187}, "predict results": {"exact_match": 4.53074433656958, "f1": 12.78303969481134, "predict_samples": 5457}}}, "layers 3": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 3.598875351452671, "f1": 10.40278139371874, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 3.8684719535783367, "f1": 11.140465207761832}, "predict results": {"exact_match": 3.598875351452671, "f1": 10.40278139371874, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 3.617619493908154, "f1": 10.698359481336539, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 2.9013539651837523, "f1": 10.322192771292544}, "predict results": {"exact_match": 3.617619493908154, "f1": 10.698359481336539, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 5.003320788133717, "f1": 12.506527043090058, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 4.6875, "f1": 11.970306313359561}, "predict results": {"exact_match": 5.003320788133717, "f1": 12.506527043090058, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 5.0046860356138705, "f1": 11.173066978031793, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 4.25531914893617, "f1": 9.78262216829786}, "predict results": {"exact_match": 5.0046860356138705, "f1": 11.173066978031793, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 5.246845251272969, "f1": 11.981878958892692, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 5.6640625, "f1": 13.475633321260421}, "predict results": {"exact_match": 5.246845251272969, "f1": 11.981878958892692, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 4.900776531492666, "f1": 11.068893725564827, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 5.7491289198606275, "f1": 11.856370996655588}, "predict results": {"exact_match": 4.900776531492666, "f1": 11.068893725564827, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 5.5777650866171715, "f1": 13.502111046616838, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 6.6, "f1": 15.12388038755733}, "predict results": {"exact_match": 5.5777650866171715, "f1": 13.502111046616838, "predict_samples": 5457}}}, "layers 4": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 5.379568884723524, "f1": 13.609449972636575, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 4.6421663442940035, "f1": 12.869299362041223}, "predict results": {"exact_match": 5.379568884723524, "f1": 13.609449972636575, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 4.273664479850047, "f1": 11.37918972839503, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 3.094777562862669, "f1": 11.364812621569914}, "predict results": {"exact_match": 4.273664479850047, "f1": 11.37918972839503, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 9.032543723710427, "f1": 17.07050612828663, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 7.8125, "f1": 16.328213366668116}, "predict results": {"exact_match": 9.032543723710427, "f1": 17.07050612828663, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 5.698219306466729, "f1": 11.709813967839674, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 5.415860735009671, "f1": 11.16710079340622}, "predict results": {"exact_match": 5.698219306466729, "f1": 11.709813967839674, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 7.504981182200575, "f1": 14.550322998289833, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 7.2265625, "f1": 14.40653785728658}, "predict results": {"exact_match": 7.504981182200575, "f1": 14.550322998289833, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 11.04400345125108, "f1": 18.52594829176018, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 11.411149825783973, "f1": 18.370690446723486}, "predict results": {"exact_match": 11.04400345125108, "f1": 18.52594829176018, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 9.366076527698459, "f1": 18.467014904094228, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 10.4, "f1": 20.86154144858654}, "predict results": {"exact_match": 9.366076527698459, "f1": 18.467014904094228, "predict_samples": 5457}}}, "layers 5": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 9.222118088097469, "f1": 19.272638797704165, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 9.671179883945841, "f1": 19.449546828452476}, "predict results": {"exact_match": 9.222118088097469, "f1": 19.272638797704165, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 7.029053420805998, "f1": 16.496914796720993, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 10.444874274661508, "f1": 18.4279874774579}, "predict results": {"exact_match": 7.029053420805998, "f1": 16.496914796720993, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 14.434359087890192, "f1": 24.260932041922885, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 13.0859375, "f1": 22.927214946138832}, "predict results": {"exact_match": 14.434359087890192, "f1": 24.260932041922885, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 6.260543580131209, "f1": 12.331806610488911, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 4.835589941972921, "f1": 11.00710313254224}, "predict results": {"exact_match": 6.260543580131209, "f1": 12.331806610488911, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 11.689174230684083, "f1": 19.54161078053086, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 11.71875, "f1": 18.94908578068114}, "predict results": {"exact_match": 11.689174230684083, "f1": 19.54161078053086, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 19.1458153580673, "f1": 28.256120889220323, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 19.860627177700348, "f1": 28.738350366262143}, "predict results": {"exact_match": 19.1458153580673, "f1": 28.256120889220323, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 13.611269750618694, "f1": 24.595318857304342, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 14.4, "f1": 26.455728447145773}, "predict results": {"exact_match": 13.611269750618694, "f1": 24.595318857304342, "predict_samples": 5457}}}, "layers 6": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 16.3261480787254, "f1": 31.02237812896639, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 15.473887814313347, "f1": 29.80046385470524}, "predict results": {"exact_match": 16.3261480787254, "f1": 31.02237812896639, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 11.790065604498594, "f1": 22.61580907562427, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 11.798839458413926, "f1": 21.639340853417906}, "predict results": {"exact_match": 11.790065604498594, "f1": 22.61580907562427, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 25.193712641133494, "f1": 38.840556420439356, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 23.046875, "f1": 35.765245218582415}, "predict results": {"exact_match": 25.193712641133494, "f1": 38.840556420439356, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 8.078725398313027, "f1": 14.505474471897177, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 9.284332688588007, "f1": 14.853816597299009}, "predict results": {"exact_match": 8.078725398313027, "f1": 14.505474471897177, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 21.053796767766215, "f1": 31.22206669359689, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 23.2421875, "f1": 33.3469532176826}, "predict results": {"exact_match": 21.053796767766215, "f1": 31.22206669359689, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 40.457290767903366, "f1": 53.51867648354654, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 40.853658536585364, "f1": 53.69234993428188}, "predict results": {"exact_match": 40.457290767903366, "f1": 53.51867648354654, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 25.985151342090234, "f1": 42.369626402250596, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 28.0, "f1": 43.00547633608074}, "predict results": {"exact_match": 25.985151342090234, "f1": 42.369626402250596, "predict_samples": 5457}}}, "layers 7": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 23.561387066541705, "f1": 41.64187688407304, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 21.8568665377176, "f1": 39.133505101035794}, "predict results": {"exact_match": 23.561387066541705, "f1": 41.64187688407304, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 20.05623242736645, "f1": 34.377009122804004, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 19.729206963249517, "f1": 32.88004236652534}, "predict results": {"exact_match": 20.05623242736645, "f1": 34.377009122804004, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 36.30728359530662, "f1": 52.33392277042067, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 30.6640625, "f1": 46.95851089249481}, "predict results": {"exact_match": 36.30728359530662, "f1": 52.33392277042067, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 7.478912839737582, "f1": 14.896052835295349, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 8.317214700193423, "f1": 15.66816095915935}, "predict results": {"exact_match": 7.478912839737582, "f1": 14.896052835295349, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 30.10847907903476, "f1": 42.611237799835095, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 33.984375, "f1": 44.598112575501105}, "predict results": {"exact_match": 30.10847907903476, "f1": 42.611237799835095, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 53.64969801553063, "f1": 67.82182585032662, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 54.00696864111498, "f1": 67.92176140753222}, "predict results": {"exact_match": 53.64969801553063, "f1": 67.82182585032662, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 35.6938892061679, "f1": 55.670554209682805, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 36.4, "f1": 56.63464483059016}, "predict results": {"exact_match": 35.6938892061679, "f1": 55.670554209682805, "predict_samples": 5457}}}, "layers 8": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 30.402999062792876, "f1": 49.15419935980739, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 29.400386847195357, "f1": 48.347886839481575}, "predict results": {"exact_match": 30.402999062792876, "f1": 49.15419935980739, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 26.82286785379569, "f1": 42.808319563062916, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 25.918762088974855, "f1": 41.285013348470244}, "predict results": {"exact_match": 26.82286785379569, "f1": 42.808319563062916, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 42.01903918529998, "f1": 58.56012467081725, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 39.6484375, "f1": 56.90678928733603}, "predict results": {"exact_match": 42.01903918529998, "f1": 58.56012467081725, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 9.540768509840674, "f1": 17.490396446608102, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 9.284332688588007, "f1": 18.270679758982432}, "predict results": {"exact_match": 9.540768509840674, "f1": 17.490396446608102, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 37.569183086119104, "f1": 50.84250509402706, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 41.6015625, "f1": 52.971750487443394}, "predict results": {"exact_match": 37.569183086119104, "f1": 50.84250509402706, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 63.07161345987921, "f1": 76.67286947296404, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 63.850174216027874, "f1": 76.74775234981466}, "predict results": {"exact_match": 63.07161345987921, "f1": 76.67286947296404, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 41.97601370645346, "f1": 63.272913662273005, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 43.8, "f1": 65.40756180671764}, "predict results": {"exact_match": 41.97601370645346, "f1": 63.272913662273005, "predict_samples": 5457}}}, "layers 9": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 32.72727272727273, "f1": 51.88546764196344, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 31.72147001934236, "f1": 50.53509121892616}, "predict results": {"exact_match": 32.72727272727273, "f1": 51.88546764196344, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 29.128397375820057, "f1": 45.21504221759859, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 28.820116054158607, "f1": 43.54402269768117}, "predict results": {"exact_match": 29.128397375820057, "f1": 45.21504221759859, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 44.54283816692495, "f1": 60.937155816335675, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 44.7265625, "f1": 61.45319604247001}, "predict results": {"exact_match": 44.54283816692495, "f1": 60.937155816335675, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 10.740393626991565, "f1": 17.970753083813978, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 10.444874274661508, "f1": 18.445813486279214}, "predict results": {"exact_match": 10.740393626991565, "f1": 17.970753083813978, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 40.46933805623201, "f1": 53.479143701283185, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 42.3828125, "f1": 54.87880685771862}, "predict results": {"exact_match": 40.46933805623201, "f1": 53.479143701283185, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 66.50560828300259, "f1": 79.91418868152293, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 66.55052264808363, "f1": 79.52784436459665}, "predict results": {"exact_match": 66.50560828300259, "f1": 79.91418868152293, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 44.41271654292785, "f1": 66.03465979573366, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 46.6, "f1": 68.09051230901984}, "predict results": {"exact_match": 44.41271654292785, "f1": 66.03465979573366, "predict_samples": 5457}}}, "layers 10": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 33.58950328022493, "f1": 52.13053260882221, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 32.30174081237911, "f1": 50.7453218227773}, "predict results": {"exact_match": 33.58950328022493, "f1": 52.13053260882221, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 29.259606373008435, "f1": 45.31004225159479, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 28.820116054158607, "f1": 44.73640528079053}, "predict results": {"exact_match": 29.259606373008435, "f1": 45.31004225159479, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 45.89329200796989, "f1": 61.47688265644326, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 44.921875, "f1": 61.554670380097456}, "predict results": {"exact_match": 45.89329200796989, "f1": 61.47688265644326, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 11.358950328022493, "f1": 17.404048744938585, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 11.411992263056092, "f1": 17.964820782044058}, "predict results": {"exact_match": 11.358950328022493, "f1": 17.404048744938585, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 41.79765330971884, "f1": 54.01367693461777, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 43.1640625, "f1": 54.78316851296784}, "predict results": {"exact_match": 41.79765330971884, "f1": 54.01367693461777, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 67.44607420189818, "f1": 80.61278849147557, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 68.46689895470384, "f1": 80.57364475488201}, "predict results": {"exact_match": 67.44607420189818, "f1": 80.61278849147557, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 45.0409289929564, "f1": 66.43947664274981, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 48.8, "f1": 69.10987339176833}, "predict results": {"exact_match": 45.0409289929564, "f1": 66.43947664274981, "predict_samples": 5457}}}, "layers 11": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 33.70196813495782, "f1": 52.216645160479914, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 32.688588007736946, "f1": 51.296141781948606}, "predict results": {"exact_match": 33.70196813495782, "f1": 52.216645160479914, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 30.00937207122774, "f1": 45.78110590745263, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 29.013539651837526, "f1": 44.18649887152343}, "predict results": {"exact_match": 30.00937207122774, "f1": 45.78110590745263, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 45.671906132388756, "f1": 61.27992918669272, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 45.1171875, "f1": 61.188421917571425}, "predict results": {"exact_match": 45.671906132388756, "f1": 61.27992918669272, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 11.565135895032801, "f1": 17.978566183885494, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 11.411992263056092, "f1": 18.31323359873947}, "predict results": {"exact_match": 11.565135895032801, "f1": 17.978566183885494, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 41.75337613460261, "f1": 54.23350774483074, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 44.3359375, "f1": 55.24326502932535}, "predict results": {"exact_match": 41.75337613460261, "f1": 54.23350774483074, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 67.63589301121657, "f1": 80.8543318622402, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 68.90243902439025, "f1": 80.88258382928775}, "predict results": {"exact_match": 67.63589301121657, "f1": 80.8543318622402, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 45.802398629354656, "f1": 66.83003617014789, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 49.0, "f1": 68.95107483207303}, "predict results": {"exact_match": 45.802398629354656, "f1": 66.83003617014789, "predict_samples": 5457}}}, "layers 12": {"mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 33.98313027179007, "f1": 52.384073010219915, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 32.688588007736946, "f1": 50.74657998378658}, "predict results": {"exact_match": 33.98313027179007, "f1": 52.384073010219915, "predict_samples": 6810}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 30.06560449859419, "f1": 46.16557354823242, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 28.820116054158607, "f1": 44.53516484805259}, "predict results": {"exact_match": 30.06560449859419, "f1": 46.16557354823242, "predict_samples": 6793}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 46.20323223378348, "f1": 61.723152656077986, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 46.2890625, "f1": 62.08207590187735}, "predict results": {"exact_match": 46.20323223378348, "f1": 61.723152656077986, "predict_samples": 5278}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 11.958762886597938, "f1": 18.89990226692123, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 12.379110251450676, "f1": 19.914583678423348}, "predict results": {"exact_match": 11.958762886597938, "f1": 18.89990226692123, "predict_samples": 7119}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 41.84193048483507, "f1": 54.40384688672063, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 44.921875, "f1": 56.180908338579414}, "predict results": {"exact_match": 41.84193048483507, "f1": 54.40384688672063, "predict_samples": 5759}}, "mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 67.73080241587576, "f1": 80.87061344600048, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 68.72822299651568, "f1": 80.85858142094278}, "predict results": {"exact_match": 67.73080241587576, "f1": 80.87061344600048, "predict_samples": 15269}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 46.24024367028365, "f1": 66.9776429987997, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 49.2, "f1": 69.41341139796079}, "predict results": {"exact_match": 46.24024367028365, "f1": 66.9776429987997, "predict_samples": 5457}}}}, "squad": {"layers 1": {"all results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 4.247871333964049, "f1": 9.729435667030057, "init_mem_cpu_alloc_delta": 1313013760, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 798026752, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 16289792, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 221829120, "train_runtime": 595.9841, "train_samples": 89597, "train_samples_per_second": 56.377}, "eval results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 4.247871333964049, "f1": 9.729435667030057}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1313013760, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 798026752, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 16289792, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 221829120, "train_runtime": 595.9841, "train_samples": 89597, "train_samples_per_second": 56.377}}, "layers 2": {"all results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 4.8344370860927155, "f1": 10.912278449247848, "init_mem_cpu_alloc_delta": 1303429120, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 826378240, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 13123584, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 906.9383, "train_samples": 89597, "train_samples_per_second": 37.048}, "eval results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 4.8344370860927155, "f1": 10.912278449247848}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1303429120, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 826378240, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 13123584, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 906.9383, "train_samples": 89597, "train_samples_per_second": 37.048}}, "layers 3": {"all results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 5.666982024597918, "f1": 11.97144607954582, "init_mem_cpu_alloc_delta": 1317892096, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 854729728, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 17035264, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 1232.2847, "train_samples": 89597, "train_samples_per_second": 27.266}, "eval results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 5.666982024597918, "f1": 11.97144607954582}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1317892096, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 854729728, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 17035264, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 1232.2847, "train_samples": 89597, "train_samples_per_second": 27.266}}, "layers 4": {"all results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 13.301797540208137, "f1": 20.84230982783611, "init_mem_cpu_alloc_delta": 1243316224, "init_mem_cpu_peaked_delta": 139264, "init_mem_gpu_alloc_delta": 883081216, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 13672448, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 1541.171, "train_samples": 89597, "train_samples_per_second": 21.802}, "eval results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 13.301797540208137, "f1": 20.84230982783611}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1243316224, "init_mem_cpu_peaked_delta": 139264, "init_mem_gpu_alloc_delta": 883081216, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 13672448, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 1541.171, "train_samples": 89597, "train_samples_per_second": 21.802}}, "layers 5": {"all results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 22.885525070955534, "f1": 31.433198269234435, "init_mem_cpu_alloc_delta": 1220431872, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 911432704, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 16736256, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 1851.4235, "train_samples": 89597, "train_samples_per_second": 18.148}, "eval results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 22.885525070955534, "f1": 31.433198269234435}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1220431872, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 911432704, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 16736256, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 1851.4235, "train_samples": 89597, "train_samples_per_second": 18.148}}, "layers 6": {"all results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 52.005676442762535, "f1": 62.848916176297976, "init_mem_cpu_alloc_delta": 1187745792, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 939784192, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 15405056, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 2170.5495, "train_samples": 89597, "train_samples_per_second": 15.48}, "eval results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 52.005676442762535, "f1": 62.848916176297976}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1187745792, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 939784192, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 15405056, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 2170.5495, "train_samples": 89597, "train_samples_per_second": 15.48}}, "layers 10": {"all results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 81.56102175969725, "f1": 88.87275916217754, "init_mem_cpu_alloc_delta": 1068687360, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 1053190144, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 15179776, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 3427.0464, "train_samples": 89597, "train_samples_per_second": 9.804}, "eval results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 81.56102175969725, "f1": 88.87275916217754}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1068687360, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 1053190144, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 15179776, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 3427.0464, "train_samples": 89597, "train_samples_per_second": 9.804}}, "layers 11": {"all results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 81.79754020813624, "f1": 89.03748155028588, "init_mem_cpu_alloc_delta": 1042653184, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 1081541632, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 11444224, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 3740.6052, "train_samples": 89597, "train_samples_per_second": 8.983}, "eval results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 81.79754020813624, "f1": 89.03748155028588}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1042653184, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 1081541632, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 11444224, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 3740.6052, "train_samples": 89597, "train_samples_per_second": 8.983}}, "layers 12": {"all results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 81.97729422894986, "f1": 89.10871301065585, "init_mem_cpu_alloc_delta": 1010626560, "init_mem_cpu_peaked_delta": 151552, "init_mem_gpu_alloc_delta": 1109893120, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 16850944, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 4060.1838, "train_samples": 89597, "train_samples_per_second": 8.275}, "eval results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 81.97729422894986, "f1": 89.10871301065585}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1010626560, "init_mem_cpu_peaked_delta": 151552, "init_mem_gpu_alloc_delta": 1109893120, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 16850944, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 4060.1838, "train_samples": 89597, "train_samples_per_second": 8.275}}, "layers 7": {"all results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 65.71428571428571, "f1": 76.05579577038698, "init_mem_cpu_alloc_delta": 1140133888, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 968135680, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 15093760, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 2463.5208, "train_samples": 89597, "train_samples_per_second": 13.639}, "eval results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 65.71428571428571, "f1": 76.05579577038698}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1140133888, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 968135680, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 15093760, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 2463.5208, "train_samples": 89597, "train_samples_per_second": 13.639}}, "layers 8": {"all results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 76.97256385998108, "f1": 85.5163981471206, "init_mem_cpu_alloc_delta": 1130139648, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 996487168, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 16973824, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 2783.4009, "train_samples": 89597, "train_samples_per_second": 12.072}, "eval results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 76.97256385998108, "f1": 85.5163981471206}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1130139648, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 996487168, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 16973824, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 2783.4009, "train_samples": 89597, "train_samples_per_second": 12.072}}, "layers 9": {"all results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 80.55818353831599, "f1": 88.28958694496548, "init_mem_cpu_alloc_delta": 1100390400, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 1024838656, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 13467648, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 3099.0131, "train_samples": 89597, "train_samples_per_second": 10.842}, "eval results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 80.55818353831599, "f1": 88.28958694496548}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1100390400, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 1024838656, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 13467648, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 52224, "train_mem_gpu_peaked_delta": 231266304, "train_runtime": 3099.0131, "train_samples": 89597, "train_samples_per_second": 10.842}}}}