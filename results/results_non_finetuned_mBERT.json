{"mlqa": {"layers 1": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 3.8425492033739457, "f1": 9.44991410822866, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 3.6750483558994196, "f1": 8.661997959184507}, "predict results": {"exact_match": 3.8425492033739457, "f1": 9.44991410822866, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 3.7300843486410495, "f1": 9.454713139085742, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 4.061895551257253, "f1": 9.13208094326525}, "predict results": {"exact_match": 3.7300843486410495, "f1": 9.454713139085742, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 4.294885986274076, "f1": 11.399292800814864, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 3.90625, "f1": 10.987564802251061}, "predict results": {"exact_match": 4.294885986274076, "f1": 11.399292800814864, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 4.386129334582943, "f1": 10.21385494549373, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 4.835589941972921, "f1": 10.549521410663834}, "predict results": {"exact_match": 4.386129334582943, "f1": 10.21385494549373, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 4.914766437901262, "f1": 11.173166675215796, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 4.8828125, "f1": 11.33310112268539}, "predict results": {"exact_match": 4.914766437901262, "f1": 11.173166675215796, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 4.219154443485763, "f1": 10.101384517157596, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 4.442508710801394, "f1": 10.19425884917036}, "predict results": {"exact_match": 4.219154443485763, "f1": 10.101384517157596, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 3.959642109270893, "f1": 11.143869173032565, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 5.2, "f1": 12.197911490030354}, "predict results": {"exact_match": 3.959642109270893, "f1": 11.143869173032565, "predict_samples": 5456}}}, "layers 2": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 4.179943767572634, "f1": 11.047639992545815, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 3.094777562862669, "f1": 10.033929660131664}, "predict results": {"exact_match": 4.179943767572634, "f1": 11.047639992545815, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 4.29240862230553, "f1": 11.10370715223307, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 2.9013539651837523, "f1": 9.898230967549665}, "predict results": {"exact_match": 4.29240862230553, "f1": 11.10370715223307, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 5.31326101394731, "f1": 12.683528834431359, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 5.2734375, "f1": 12.617325209532156}, "predict results": {"exact_match": 5.31326101394731, "f1": 12.683528834431359, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 5.023430178069353, "f1": 11.073733436452871, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 5.609284332688588, "f1": 12.387659107509618}, "predict results": {"exact_match": 5.023430178069353, "f1": 11.073733436452871, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 5.6232012397609035, "f1": 12.402359556703141, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 5.2734375, "f1": 12.165552783439258}, "predict results": {"exact_match": 5.6232012397609035, "f1": 12.402359556703141, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 4.762726488352028, "f1": 10.979922715266346, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 4.616724738675958, "f1": 10.646879046972359}, "predict results": {"exact_match": 4.762726488352028, "f1": 10.979922715266346, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 4.721111745669141, "f1": 13.33286307816555, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 4.8, "f1": 13.470004970934943}, "predict results": {"exact_match": 4.721111745669141, "f1": 13.33286307816555, "predict_samples": 5456}}}, "layers 3": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 4.57357075913777, "f1": 10.995308922628775, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 3.481624758220503, "f1": 10.302043639352698}, "predict results": {"exact_match": 4.57357075913777, "f1": 10.995308922628775, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 4.6485473289597, "f1": 11.075439176642838, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 3.094777562862669, "f1": 9.612850825883724}, "predict results": {"exact_match": 4.6485473289597, "f1": 11.075439176642838, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 5.468231126854107, "f1": 12.569227173258557, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 4.4921875, "f1": 11.25856322533345}, "predict results": {"exact_match": 5.468231126854107, "f1": 12.569227173258557, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 5.360824742268041, "f1": 11.160208526901274, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 5.609284332688588, "f1": 11.573884779636757}, "predict results": {"exact_match": 5.360824742268041, "f1": 11.160208526901274, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 6.220943103829976, "f1": 12.53815991935373, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 5.46875, "f1": 11.914945100401141}, "predict results": {"exact_match": 6.220943103829976, "f1": 12.53815991935373, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 5.6341673856773085, "f1": 11.43074423816513, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 4.616724738675958, "f1": 10.773559081689275}, "predict results": {"exact_match": 5.6341673856773085, "f1": 11.43074423816513, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 5.863316200266515, "f1": 14.21604930860689, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 6.4, "f1": 15.355160073507975}, "predict results": {"exact_match": 5.863316200266515, "f1": 14.21604930860689, "predict_samples": 5456}}}, "layers 4": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 5.042174320524836, "f1": 11.097872393576004, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 4.835589941972921, "f1": 11.812186619804336}, "predict results": {"exact_match": 5.042174320524836, "f1": 11.097872393576004, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 4.967197750702906, "f1": 11.100734465747527, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 4.25531914893617, "f1": 11.499750605149568}, "predict results": {"exact_match": 4.967197750702906, "f1": 11.100734465747527, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 6.818684967899048, "f1": 13.781697470116663, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 7.2265625, "f1": 13.676207536081566}, "predict results": {"exact_match": 6.818684967899048, "f1": 13.781697470116663, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 6.148078725398313, "f1": 11.581198784113099, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 6.382978723404255, "f1": 12.175654053034124}, "predict results": {"exact_match": 6.148078725398313, "f1": 11.581198784113099, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 7.327872481735665, "f1": 13.232640110417229, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 6.640625, "f1": 13.625913360006711}, "predict results": {"exact_match": 7.327872481735665, "f1": 13.232640110417229, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 6.678170836928387, "f1": 12.179398170470453, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 6.358885017421603, "f1": 12.332567390481685}, "predict results": {"exact_match": 6.678170836928387, "f1": 12.179398170470453, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 6.872263468494194, "f1": 14.755982093998057, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 8.0, "f1": 16.521457294725206}, "predict results": {"exact_match": 6.872263468494194, "f1": 14.755982093998057, "predict_samples": 5456}}}, "layers 5": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 5.754451733833177, "f1": 12.388673625473695, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 5.609284332688588, "f1": 12.064366324594669}, "predict results": {"exact_match": 5.754451733833177, "f1": 12.388673625473695, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 5.229615745079663, "f1": 11.425782544640521, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 4.061895551257253, "f1": 10.409600310774561}, "predict results": {"exact_match": 5.229615745079663, "f1": 11.425782544640521, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 7.992030108479079, "f1": 15.32224408838123, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 8.0078125, "f1": 14.44197668592411}, "predict results": {"exact_match": 7.992030108479079, "f1": 15.32224408838123, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 6.148078725398313, "f1": 11.973777616583188, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 5.802707930367505, "f1": 10.63751963109482}, "predict results": {"exact_match": 6.148078725398313, "f1": 11.973777616583188, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 8.744742085454948, "f1": 14.992521818561057, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 7.03125, "f1": 12.76542592738694}, "predict results": {"exact_match": 8.744742085454948, "f1": 14.992521818561057, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 8.792062122519413, "f1": 15.056069860859173, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 8.362369337979095, "f1": 14.400542101785996}, "predict results": {"exact_match": 8.792062122519413, "f1": 15.056069860859173, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 8.128688368551304, "f1": 16.86337758978659, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 9.6, "f1": 17.901354803785107}, "predict results": {"exact_match": 8.128688368551304, "f1": 16.86337758978659, "predict_samples": 5456}}}, "layers 6": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 8.940955951265229, "f1": 17.370803013035022, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 10.251450676982591, "f1": 17.882632361841438}, "predict results": {"exact_match": 8.940955951265229, "f1": 17.370803013035022, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 6.916588566073102, "f1": 13.525643157739372, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 6.576402321083172, "f1": 12.734275305986346}, "predict results": {"exact_match": 6.916588566073102, "f1": 13.525643157739372, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 11.866282931148993, "f1": 20.45763616283074, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 13.0859375, "f1": 21.619810494158017}, "predict results": {"exact_match": 11.866282931148993, "f1": 20.45763616283074, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 7.685098406747891, "f1": 13.443860136798016, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 7.350096711798839, "f1": 12.198708084644528}, "predict results": {"exact_match": 7.685098406747891, "f1": 13.443860136798016, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 10.471551914987824, "f1": 17.675654276781465, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 9.5703125, "f1": 16.38606935156318}, "predict results": {"exact_match": 10.471551914987824, "f1": 17.675654276781465, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 14.314063848144952, "f1": 22.32617087468177, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 13.937282229965156, "f1": 22.03383999250329}, "predict results": {"exact_match": 14.314063848144952, "f1": 22.32617087468177, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 11.36493432324386, "f1": 21.807179974283354, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 11.8, "f1": 22.521698413538605}, "predict results": {"exact_match": 11.36493432324386, "f1": 21.807179974283354, "predict_samples": 5456}}}, "layers 7": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 11.69634489222118, "f1": 21.468094462877808, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 9.864603481624759, "f1": 18.811278013076866}, "predict results": {"exact_match": 11.69634489222118, "f1": 21.468094462877808, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 7.179006560449859, "f1": 14.134522838813412, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 6.769825918762089, "f1": 13.895024697775913}, "predict results": {"exact_match": 7.179006560449859, "f1": 14.134522838813412, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 15.253486827540403, "f1": 25.372608325392527, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 13.671875, "f1": 22.386856709890825}, "predict results": {"exact_match": 15.253486827540403, "f1": 25.372608325392527, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 8.30365510777882, "f1": 14.153196727518653, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 8.897485493230175, "f1": 14.331736275001521}, "predict results": {"exact_match": 8.30365510777882, "f1": 14.153196727518653, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 14.611467788355103, "f1": 21.721270530459122, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 14.0625, "f1": 21.90755755105921}, "predict results": {"exact_match": 14.611467788355103, "f1": 21.721270530459122, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 20.578084555651422, "f1": 30.276645497336474, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 20.209059233449477, "f1": 29.431561499313958}, "predict results": {"exact_match": 20.578084555651422, "f1": 30.276645497336474, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 15.400723396154579, "f1": 26.83420028120706, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 14.4, "f1": 25.523582481035028}, "predict results": {"exact_match": 15.400723396154579, "f1": 26.83420028120706, "predict_samples": 5456}}}, "layers 8": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 11.69634489222118, "f1": 21.62791719601215, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 9.477756286266924, "f1": 19.96321898097019}, "predict results": {"exact_match": 11.69634489222118, "f1": 21.62791719601215, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 6.954076850984068, "f1": 14.180598633848952, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 7.156673114119923, "f1": 15.525909796257304}, "predict results": {"exact_match": 6.954076850984068, "f1": 14.180598633848952, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 15.784812928935134, "f1": 25.700016942079028, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 12.5, "f1": 22.054425617118856}, "predict results": {"exact_match": 15.784812928935134, "f1": 25.700016942079028, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 8.134957825679475, "f1": 14.125096432416806, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 9.477756286266924, "f1": 14.671417841546647}, "predict results": {"exact_match": 8.134957825679475, "f1": 14.125096432416806, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 14.810715076378127, "f1": 22.42674507726765, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 13.28125, "f1": 21.709928621029693}, "predict results": {"exact_match": 14.810715076378127, "f1": 22.42674507726765, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 21.00086281276963, "f1": 30.781482645698127, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 20.29616724738676, "f1": 30.268197020392037}, "predict results": {"exact_match": 21.00086281276963, "f1": 30.781482645698127, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 15.09613554159528, "f1": 27.20590575984853, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 13.4, "f1": 25.249658095859605}, "predict results": {"exact_match": 15.09613554159528, "f1": 27.20590575984853, "predict_samples": 5456}}}, "layers 9": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 13.477038425492033, "f1": 24.800527371857854, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 13.346228239845262, "f1": 23.880441906667766}, "predict results": {"exact_match": 13.477038425492033, "f1": 24.800527371857854, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 9.484536082474227, "f1": 18.13366229137016, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 10.058027079303676, "f1": 18.07389631540622}, "predict results": {"exact_match": 9.484536082474227, "f1": 18.13366229137016, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 18.884215187071064, "f1": 31.071209055619324, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 16.796875, "f1": 27.930069709865556}, "predict results": {"exact_match": 18.884215187071064, "f1": 31.071209055619324, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 9.184629803186505, "f1": 16.116238020719724, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 10.058027079303676, "f1": 17.42647681649059}, "predict results": {"exact_match": 9.184629803186505, "f1": 16.116238020719724, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 17.68873145893292, "f1": 26.969666245375734, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 18.5546875, "f1": 29.3292994446438}, "predict results": {"exact_match": 17.68873145893292, "f1": 26.969666245375734, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 24.987057808455564, "f1": 36.42656521421118, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 25.174216027874564, "f1": 37.48243030923035}, "predict results": {"exact_match": 24.987057808455564, "f1": 36.42656521421118, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 18.046830382638493, "f1": 32.097924853045676, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 19.2, "f1": 32.77630584788218}, "predict results": {"exact_match": 18.046830382638493, "f1": 32.097924853045676, "predict_samples": 5456}}}, "layers 10": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 12.427366447985005, "f1": 23.85696550411473, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 11.411992263056092, "f1": 21.513379033455212}, "predict results": {"exact_match": 12.427366447985005, "f1": 23.85696550411473, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 8.359887535145267, "f1": 16.962417254298632, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 8.317214700193423, "f1": 16.52698644360045}, "predict results": {"exact_match": 8.359887535145267, "f1": 16.962417254298632, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 17.356652645561212, "f1": 29.541943236957227, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 15.625, "f1": 27.811016784213976}, "predict results": {"exact_match": 17.356652645561212, "f1": 29.541943236957227, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 9.334582942830366, "f1": 16.3927532092434, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 10.444874274661508, "f1": 17.667481751950127}, "predict results": {"exact_match": 9.334582942830366, "f1": 16.3927532092434, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 16.913880894398936, "f1": 26.441331930632877, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 16.6015625, "f1": 27.494957650615948}, "predict results": {"exact_match": 16.913880894398936, "f1": 26.441331930632877, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 23.86540120793788, "f1": 35.42007451247839, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 24.041811846689896, "f1": 36.432206998261776}, "predict results": {"exact_match": 23.86540120793788, "f1": 35.42007451247839, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 17.11403007805064, "f1": 30.89733337671776, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 18.4, "f1": 32.073584324054686}, "predict results": {"exact_match": 17.11403007805064, "f1": 30.89733337671776, "predict_samples": 5456}}}, "layers 11": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 11.865042174320525, "f1": 23.273637509469257, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 11.992263056092844, "f1": 22.971382648530167}, "predict results": {"exact_match": 11.865042174320525, "f1": 23.273637509469257, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 7.966260543580131, "f1": 16.478408169899186, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 8.123791102514506, "f1": 16.595953114670294}, "predict results": {"exact_match": 7.966260543580131, "f1": 16.478408169899186, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 16.29400044277175, "f1": 28.63104168049794, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 17.3828125, "f1": 28.681124163947185}, "predict results": {"exact_match": 16.29400044277175, "f1": 28.63104168049794, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 9.14714151827554, "f1": 16.088441598080895, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 9.671179883945841, "f1": 17.318386694632483}, "predict results": {"exact_match": 9.14714151827554, "f1": 16.088441598080895, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 16.40469338056232, "f1": 25.857930812203193, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 15.8203125, "f1": 26.53847240483578}, "predict results": {"exact_match": 16.40469338056232, "f1": 25.857930812203193, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 22.97670405522002, "f1": 34.30528644242514, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 23.519163763066203, "f1": 34.96837682952921}, "predict results": {"exact_match": 22.97670405522002, "f1": 34.30528644242514, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 16.50485436893204, "f1": 30.237501873533414, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 18.8, "f1": 31.647310853901633}, "predict results": {"exact_match": 16.50485436893204, "f1": 30.237501873533414, "predict_samples": 5456}}}, "layers 12": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 9.559512652296158, "f1": 20.568545932452473, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 8.897485493230175, "f1": 18.675521070637597}, "predict results": {"exact_match": 9.559512652296158, "f1": 20.568545932452473, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 6.4667291471415185, "f1": 15.17149874178308, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 6.963249516441006, "f1": 15.237153018317287}, "predict results": {"exact_match": 6.4667291471415185, "f1": 15.17149874178308, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 12.928935133938454, "f1": 24.204328112052966, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 10.9375, "f1": 23.05835424727224}, "predict results": {"exact_match": 12.928935133938454, "f1": 24.204328112052966, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 7.572633552014995, "f1": 15.003058804578865, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 8.897485493230175, "f1": 16.538231493269432}, "predict results": {"exact_match": 7.572633552014995, "f1": 15.003058804578865, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 12.596856320566747, "f1": 21.93675435468362, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 12.3046875, "f1": 22.316914621748566}, "predict results": {"exact_match": 12.596856320566747, "f1": 21.93675435468362, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 17.782571182053495, "f1": 28.7385541182112, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 17.334494773519165, "f1": 28.813855752670037}, "predict results": {"exact_match": 17.782571182053495, "f1": 28.7385541182112, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 13.040167523320008, "f1": 24.88197984272484, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 12.2, "f1": 24.364172363710086}, "predict results": {"exact_match": 13.040167523320008, "f1": 24.88197984272484, "predict_samples": 5456}}}}, "squad": {"layers 1": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 4.7303689687795645, "f1": 10.497770052469267, "init_mem_cpu_alloc_delta": 1736994816, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 397473280, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 14401536, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 221786112, "train_runtime": 678.028, "train_samples": 88880, "train_samples_per_second": 49.157}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 4.7303689687795645, "f1": 10.497770052469267}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1736994816, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 397473280, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 14401536, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 221786112, "train_runtime": 678.028, "train_samples": 88880, "train_samples_per_second": 49.157}}, "layers 2": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 5.203405865657522, "f1": 11.257658339367731, "init_mem_cpu_alloc_delta": 1726390272, "init_mem_cpu_peaked_delta": 114688, "init_mem_gpu_alloc_delta": 425824768, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 13840384, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 1005.9033, "train_samples": 88880, "train_samples_per_second": 33.134}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 5.203405865657522, "f1": 11.257658339367731}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1726390272, "init_mem_cpu_peaked_delta": 114688, "init_mem_gpu_alloc_delta": 425824768, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 13840384, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 1005.9033, "train_samples": 88880, "train_samples_per_second": 33.134}}, "layers 3": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 6.111636707663198, "f1": 11.941704786732581, "init_mem_cpu_alloc_delta": 2322161664, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 454176256, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 15863808, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 758.996, "train_samples": 88880, "train_samples_per_second": 43.913}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 6.111636707663198, "f1": 11.941704786732581}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2322161664, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 454176256, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 15863808, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 758.996, "train_samples": 88880, "train_samples_per_second": 43.913}}, "layers 4": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 7.634815515610217, "f1": 13.08592135062958, "init_mem_cpu_alloc_delta": 2281238528, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 482527744, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 15982592, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 938.1892, "train_samples": 88880, "train_samples_per_second": 35.526}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 7.634815515610217, "f1": 13.08592135062958}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2281238528, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 482527744, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 15982592, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 938.1892, "train_samples": 88880, "train_samples_per_second": 35.526}}, "layers 5": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 10.671712393566699, "f1": 16.858569005287087, "init_mem_cpu_alloc_delta": 2286026752, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 510879232, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 9310208, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 3381.3871, "train_samples": 88880, "train_samples_per_second": 9.857}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 10.671712393566699, "f1": 16.858569005287087}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2286026752, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 510879232, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 9310208, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 3381.3871, "train_samples": 88880, "train_samples_per_second": 9.857}}, "layers 6": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 19.262062440870388, "f1": 28.17900318298613, "init_mem_cpu_alloc_delta": 2246148096, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 539230720, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 12591104, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 4056.8827, "train_samples": 88880, "train_samples_per_second": 8.216}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 19.262062440870388, "f1": 28.17900318298613}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2246148096, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 539230720, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 12591104, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 4056.8827, "train_samples": 88880, "train_samples_per_second": 8.216}}, "layers 7": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 31.050141911069062, "f1": 40.61274872664219, "init_mem_cpu_alloc_delta": 2220343296, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 567582208, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 14966784, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 1474.0022, "train_samples": 88880, "train_samples_per_second": 22.612}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 31.050141911069062, "f1": 40.61274872664219}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2220343296, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 567582208, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 14966784, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 1474.0022, "train_samples": 88880, "train_samples_per_second": 22.612}}, "layers 8": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 30.97445600756859, "f1": 41.07642801473577, "init_mem_cpu_alloc_delta": 2173415424, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 595933696, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 18919424, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 1629.5068, "train_samples": 88880, "train_samples_per_second": 20.454}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 30.97445600756859, "f1": 41.07642801473577}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2173415424, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 595933696, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 18919424, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 1629.5068, "train_samples": 88880, "train_samples_per_second": 20.454}}, "layers 9": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 35.88457899716178, "f1": 46.99782566878518, "init_mem_cpu_alloc_delta": 2146373632, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 624285184, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 10117120, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 5916.0339, "train_samples": 88880, "train_samples_per_second": 5.634}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 35.88457899716178, "f1": 46.99782566878518}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2146373632, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 624285184, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 10117120, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 5916.0339, "train_samples": 88880, "train_samples_per_second": 5.634}}, "layers 10": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 34.46546830652791, "f1": 45.761330292267296, "init_mem_cpu_alloc_delta": 1477677056, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 652636672, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 15572992, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 3452.1033, "train_samples": 88880, "train_samples_per_second": 9.655}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 34.46546830652791, "f1": 45.761330292267296}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1477677056, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 652636672, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 15572992, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 3452.1033, "train_samples": 88880, "train_samples_per_second": 9.655}}, "layers 11": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 33.09366130558183, "f1": 44.283337820177074, "init_mem_cpu_alloc_delta": 1426923520, "init_mem_cpu_peaked_delta": 229376, "init_mem_gpu_alloc_delta": 680988160, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 14745600, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 3762.189, "train_samples": 88880, "train_samples_per_second": 8.859}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 33.09366130558183, "f1": 44.283337820177074}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1426923520, "init_mem_cpu_peaked_delta": 229376, "init_mem_gpu_alloc_delta": 680988160, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 14745600, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 3762.189, "train_samples": 88880, "train_samples_per_second": 8.859}}, "layers 12": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 25.912961210974455, "f1": 36.73219743674523, "init_mem_cpu_alloc_delta": 1425674240, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 709339648, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 15204352, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 4090.4601, "train_samples": 88880, "train_samples_per_second": 8.148}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 25.912961210974455, "f1": 36.73219743674523}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1425674240, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 709339648, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 15204352, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 4090.4601, "train_samples": 88880, "train_samples_per_second": 8.148}}}}