{"mlqa": {"layers 1": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 3.973758200562324, "f1": 9.799605762935094, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 3.6750483558994196, "f1": 9.318406076833051}, "predict results": {"exact_match": 3.973758200562324, "f1": 9.799605762935094, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 4.01124648547329, "f1": 9.561921959055637, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 4.25531914893617, "f1": 9.446726295756953}, "predict results": {"exact_match": 4.01124648547329, "f1": 9.561921959055637, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 4.007084348018596, "f1": 11.55807464223856, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 3.515625, "f1": 11.238043012793824}, "predict results": {"exact_match": 4.007084348018596, "f1": 11.55807464223856, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 4.423617619493908, "f1": 10.515652888900485, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 5.029013539651838, "f1": 11.349644882370916}, "predict results": {"exact_match": 4.423617619493908, "f1": 10.515652888900485, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 5.0697365508080585, "f1": 11.336498529019751, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 4.8828125, "f1": 11.450734932326617}, "predict results": {"exact_match": 5.0697365508080585, "f1": 11.336498529019751, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 4.184641932700604, "f1": 10.276058122430923, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 4.355400696864112, "f1": 10.552157526832342}, "predict results": {"exact_match": 4.184641932700604, "f1": 10.276058122430923, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 3.997715591090805, "f1": 11.268975913236682, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 4.6, "f1": 12.196705993962732}, "predict results": {"exact_match": 3.997715591090805, "f1": 11.268975913236682, "predict_samples": 5456}}}, "layers 2": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 4.1986879100281165, "f1": 10.360474438319413, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 3.8684719535783367, "f1": 9.76090847798292}, "predict results": {"exact_match": 4.1986879100281165, "f1": 10.360474438319413, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 4.329896907216495, "f1": 10.616857875231014, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 3.288201160541586, "f1": 9.958190930632428}, "predict results": {"exact_match": 4.329896907216495, "f1": 10.616857875231014, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 4.405578924064645, "f1": 12.22626169829973, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 4.8828125, "f1": 12.313223157891294}, "predict results": {"exact_match": 4.405578924064645, "f1": 12.22626169829973, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 4.873477038425492, "f1": 11.156038628693357, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 5.802707930367505, "f1": 12.202088127520401}, "predict results": {"exact_match": 4.873477038425492, "f1": 11.156038628693357, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 5.357538189063538, "f1": 12.16414244732785, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 4.6875, "f1": 11.880638247264004}, "predict results": {"exact_match": 5.357538189063538, "f1": 12.16414244732785, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 4.607420189818809, "f1": 10.87678030016392, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 4.878048780487805, "f1": 11.312879305138502}, "predict results": {"exact_match": 4.607420189818809, "f1": 10.87678030016392, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 4.835332191128879, "f1": 13.45914071926585, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 4.6, "f1": 13.355511751606521}, "predict results": {"exact_match": 4.835332191128879, "f1": 13.45914071926585, "predict_samples": 5456}}}, "layers 3": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 4.723523898781631, "f1": 11.316157301458553, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 3.481624758220503, "f1": 9.95819355622565}, "predict results": {"exact_match": 4.723523898781631, "f1": 11.316157301458553, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 4.5923149015932525, "f1": 11.209980916156876, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 3.6750483558994196, "f1": 10.8365860669575}, "predict results": {"exact_match": 4.5923149015932525, "f1": 11.209980916156876, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 5.0697365508080585, "f1": 12.532916486639321, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 4.296875, "f1": 12.088879111077905}, "predict results": {"exact_match": 5.0697365508080585, "f1": 12.532916486639321, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 5.360824742268041, "f1": 11.5010658272631, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 4.6421663442940035, "f1": 10.571621383170067}, "predict results": {"exact_match": 5.360824742268041, "f1": 11.5010658272631, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 6.065972990923179, "f1": 12.517612830890714, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 5.2734375, "f1": 12.061018503775694}, "predict results": {"exact_match": 6.065972990923179, "f1": 12.517612830890714, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 5.220017256255392, "f1": 11.325068969393667, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 4.442508710801394, "f1": 10.36649542455491}, "predict results": {"exact_match": 5.220017256255392, "f1": 11.325068969393667, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 5.787169236626689, "f1": 14.56196687878265, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 6.6, "f1": 14.941488543318258}, "predict results": {"exact_match": 5.787169236626689, "f1": 14.56196687878265, "predict_samples": 5456}}}, "layers 4": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 5.1358950328022495, "f1": 11.748110922342606, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 4.448742746615087, "f1": 12.30233322384698}, "predict results": {"exact_match": 5.1358950328022495, "f1": 11.748110922342606, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 5.5295220243673855, "f1": 12.297502388072253, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 5.222437137330754, "f1": 13.042592681591733}, "predict results": {"exact_match": 5.5295220243673855, "f1": 12.297502388072253, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 7.925614345804737, "f1": 15.399946434886605, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 7.2265625, "f1": 15.000652198024463}, "predict results": {"exact_match": 7.925614345804737, "f1": 15.399946434886605, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 6.279287722586692, "f1": 12.082443754052516, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 5.222437137330754, "f1": 11.685299451095405}, "predict results": {"exact_match": 6.279287722586692, "f1": 12.082443754052516, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 7.52711976975869, "f1": 13.974754154093537, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 7.421875, "f1": 13.363268101573015}, "predict results": {"exact_match": 7.52711976975869, "f1": 13.974754154093537, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 7.7998274374460745, "f1": 14.492577163740776, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 7.7526132404181185, "f1": 15.256176207058594}, "predict results": {"exact_match": 7.7998274374460745, "f1": 14.492577163740776, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 7.062630877593756, "f1": 15.808729420207142, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 8.2, "f1": 17.260215733691254}, "predict results": {"exact_match": 7.062630877593756, "f1": 15.808729420207142, "predict_samples": 5456}}}, "layers 5": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 8.472352389878163, "f1": 18.01363795652155, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 8.123791102514506, "f1": 17.004751589748924}, "predict results": {"exact_match": 8.472352389878163, "f1": 18.01363795652155, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 7.553889409559512, "f1": 16.323685277839562, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 7.93036750483559, "f1": 16.42238632966265}, "predict results": {"exact_match": 7.553889409559512, "f1": 16.323685277839562, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 14.168696037192827, "f1": 24.55697883781077, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 11.71875, "f1": 21.401814089925036}, "predict results": {"exact_match": 14.168696037192827, "f1": 24.55697883781077, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 7.441424554826616, "f1": 14.435298458225413, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 6.769825918762089, "f1": 13.944220192362454}, "predict results": {"exact_match": 7.441424554826616, "f1": 14.435298458225413, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 12.242638919636928, "f1": 21.175555906740414, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 11.5234375, "f1": 20.376451236970734}, "predict results": {"exact_match": 12.242638919636928, "f1": 21.175555906740414, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 19.577221742881793, "f1": 29.992292717065837, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 17.857142857142858, "f1": 28.48444455886808}, "predict results": {"exact_match": 19.577221742881793, "f1": 29.992292717065837, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 13.478012564249001, "f1": 25.444093356084572, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 13.6, "f1": 26.000895000349725}, "predict results": {"exact_match": 13.478012564249001, "f1": 25.444093356084572, "predict_samples": 5456}}}, "layers 6": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 13.814432989690722, "f1": 25.977947190720354, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 13.926499032882012, "f1": 26.03245938469932}, "predict results": {"exact_match": 13.814432989690722, "f1": 25.977947190720354, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 12.03373945641987, "f1": 22.09790020390049, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 13.733075435203094, "f1": 23.06022895689166}, "predict results": {"exact_match": 12.03373945641987, "f1": 22.09790020390049, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 21.1866282931149, "f1": 33.19234813239536, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 19.3359375, "f1": 29.830249853719735}, "predict results": {"exact_match": 21.1866282931149, "f1": 33.19234813239536, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 12.5398313027179, "f1": 20.532845691963708, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 13.926499032882012, "f1": 20.891825184861425}, "predict results": {"exact_match": 12.5398313027179, "f1": 20.532845691963708, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 22.847022359973433, "f1": 33.452740400057515, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 20.3125, "f1": 30.453490242122765}, "predict results": {"exact_match": 22.847022359973433, "f1": 33.452740400057515, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 34.11561691113028, "f1": 47.30718768414305, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 34.146341463414636, "f1": 47.45766042704661}, "predict results": {"exact_match": 34.11561691113028, "f1": 47.30718768414305, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 21.340186560060918, "f1": 36.57260567669723, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 23.2, "f1": 37.2707691210224}, "predict results": {"exact_match": 21.340186560060918, "f1": 36.57260567669723, "predict_samples": 5456}}}, "layers 7": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 16.663542642924085, "f1": 29.50414129902366, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 18.568665377176014, "f1": 31.204114306255246}, "predict results": {"exact_match": 16.663542642924085, "f1": 29.50414129902366, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 14.414245548266166, "f1": 24.84931870043169, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 14.700193423597678, "f1": 24.515484723966708}, "predict results": {"exact_match": 14.414245548266166, "f1": 24.84931870043169, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 24.04250608811158, "f1": 37.15713825346298, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 21.2890625, "f1": 32.19764230481442}, "predict results": {"exact_match": 24.04250608811158, "f1": 37.15713825346298, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 14.78912839737582, "f1": 22.739712455349768, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 16.827852998065765, "f1": 25.34005982858776}, "predict results": {"exact_match": 14.78912839737582, "f1": 22.739712455349768, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 28.42594642461811, "f1": 39.13494025213459, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 27.734375, "f1": 38.343608398282015}, "predict results": {"exact_match": 28.42594642461811, "f1": 39.13494025213459, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 40.97497842968076, "f1": 53.78476741326948, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 40.41811846689895, "f1": 53.439978731113094}, "predict results": {"exact_match": 40.97497842968076, "f1": 53.78476741326948, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 25.566343042071196, "f1": 41.47966849385123, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 26.2, "f1": 42.608883681655655}, "predict results": {"exact_match": 25.566343042071196, "f1": 41.47966849385123, "predict_samples": 5456}}}, "layers 8": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 19.850046860356137, "f1": 33.5423664713798, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 19.922630560928432, "f1": 34.317614366595244}, "predict results": {"exact_match": 19.850046860356137, "f1": 33.5423664713798, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 16.363636363636363, "f1": 28.424267956275735, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 17.60154738878143, "f1": 29.631796833814406}, "predict results": {"exact_match": 16.363636363636363, "f1": 28.424267956275735, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 27.141908346247508, "f1": 41.37211431135374, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 24.609375, "f1": 37.04186756231792}, "predict results": {"exact_match": 27.141908346247508, "f1": 41.37211431135374, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 16.7572633552015, "f1": 25.73792056657054, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 20.309477756286267, "f1": 29.277070585571224}, "predict results": {"exact_match": 16.7572633552015, "f1": 25.73792056657054, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 31.901704671241976, "f1": 43.82375479991638, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 31.4453125, "f1": 42.619995603995434}, "predict results": {"exact_match": 31.901704671241976, "f1": 43.82375479991638, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 45.98792062122519, "f1": 59.52313367728025, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 44.94773519163763, "f1": 58.62829284104431}, "predict results": {"exact_match": 45.98792062122519, "f1": 59.52313367728025, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 28.631258328574148, "f1": 45.80658953261663, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 29.8, "f1": 48.14490937441851}, "predict results": {"exact_match": 28.631258328574148, "f1": 45.80658953261663, "predict_samples": 5456}}}, "layers 9": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 24.742268041237114, "f1": 41.73225375002715, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 24.564796905222437, "f1": 42.302640741713965}, "predict results": {"exact_match": 24.742268041237114, "f1": 41.73225375002715, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 28.09746954076851, "f1": 45.02741573807606, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 29.013539651837526, "f1": 45.990237146433905}, "predict results": {"exact_match": 28.09746954076851, "f1": 45.02741573807606, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 38.27761788797875, "f1": 54.99363631692344, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 34.765625, "f1": 52.0320979434303}, "predict results": {"exact_match": 38.27761788797875, "f1": 54.99363631692344, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 23.692596063730086, "f1": 37.31596793307746, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 25.918762088974855, "f1": 37.77506403379199}, "predict results": {"exact_match": 23.692596063730086, "f1": 37.31596793307746, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 44.232897941111354, "f1": 59.273706830301414, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 44.140625, "f1": 59.98128083134217}, "predict results": {"exact_match": 44.232897941111354, "f1": 59.273706830301414, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 61.83779119930975, "f1": 75.89527526931906, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 62.63066202090592, "f1": 76.01211951265401}, "predict results": {"exact_match": 61.83779119930975, "f1": 75.89527526931906, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 38.87302493813059, "f1": 60.35922261230747, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 44.0, "f1": 64.35084704471721}, "predict results": {"exact_match": 38.87302493813059, "f1": 60.35922261230747, "predict_samples": 5456}}}, "layers 10": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 26.54170571696345, "f1": 44.27964067900927, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 25.72533849129594, "f1": 44.36437980983047}, "predict results": {"exact_match": 26.54170571696345, "f1": 44.27964067900927, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 32.74601686972821, "f1": 50.43676177190693, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 32.10831721470019, "f1": 49.61599736185627}, "predict results": {"exact_match": 32.74601686972821, "f1": 50.43676177190693, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 42.35111799867168, "f1": 58.5710994141833, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 40.0390625, "f1": 57.36169975742788}, "predict results": {"exact_match": 42.35111799867168, "f1": 58.5710994141833, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 27.066541705716965, "f1": 41.22566080936148, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 30.754352030947775, "f1": 43.60079745586488}, "predict results": {"exact_match": 27.066541705716965, "f1": 41.22566080936148, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 49.65685189284924, "f1": 64.30325380754897, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 49.0234375, "f1": 63.50464284149464}, "predict results": {"exact_match": 49.65685189284924, "f1": 64.30325380754897, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 65.8067299396031, "f1": 79.4403727999559, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 64.89547038327527, "f1": 78.59055709622459}, "predict results": {"exact_match": 65.8067299396031, "f1": 79.4403727999559, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 42.375785265562534, "f1": 63.96233166986046, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 45.2, "f1": 67.16292873139113}, "predict results": {"exact_match": 42.375785265562534, "f1": 63.96233166986046, "predict_samples": 5456}}}, "layers 11": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 28.02249297094658, "f1": 44.96849449879105, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 29.013539651837526, "f1": 44.578795784203436}, "predict results": {"exact_match": 28.02249297094658, "f1": 44.96849449879105, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 33.720712277413305, "f1": 51.098898346616316, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 34.23597678916828, "f1": 51.08040523663441}, "predict results": {"exact_match": 33.720712277413305, "f1": 51.098898346616316, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 43.39163161390303, "f1": 59.11018736453241, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 42.7734375, "f1": 59.537067538745056}, "predict results": {"exact_match": 43.39163161390303, "f1": 59.11018736453241, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 29.540768509840674, "f1": 42.739645736180265, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 32.495164410058024, "f1": 44.67555138994371}, "predict results": {"exact_match": 29.540768509840674, "f1": 42.739645736180265, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 51.472216072614565, "f1": 65.7285176353563, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 49.8046875, "f1": 63.4970122457234}, "predict results": {"exact_match": 51.472216072614565, "f1": 65.7285176353563, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 66.63503019844694, "f1": 80.1154746844377, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 66.28919860627178, "f1": 79.01576031743444}, "predict results": {"exact_match": 66.63503019844694, "f1": 80.1154746844377, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 43.57509994288978, "f1": 64.96199616282814, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 46.0, "f1": 67.48490196753279}, "predict results": {"exact_match": 43.57509994288978, "f1": 64.96199616282814, "predict_samples": 5456}}}, "layers 12": {"mlqa_ar_ar": {"all results": {"eval_samples": 747, "exact_match": 27.94751640112465, "f1": 44.93746626160764, "predict_samples": 7534}, "eval results": {"eval_samples": 747, "exact_match": 29.78723404255319, "f1": 45.00000176964877}, "predict results": {"exact_match": 27.94751640112465, "f1": 44.93746626160764, "predict_samples": 7534}}, "mlqa_ar_en": {"all results": {"eval_samples": 742, "exact_match": 33.81443298969072, "f1": 51.197173033518524, "predict_samples": 7470}, "eval results": {"eval_samples": 742, "exact_match": 35.009671179883945, "f1": 51.46626518899795}, "predict results": {"exact_match": 33.81443298969072, "f1": 51.197173033518524, "predict_samples": 7470}}, "mlqa_de_de": {"all results": {"eval_samples": 583, "exact_match": 43.61301748948417, "f1": 59.39188237621245, "predict_samples": 5242}, "eval results": {"eval_samples": 583, "exact_match": 41.40625, "f1": 58.65211746682697}, "predict results": {"exact_match": 43.61301748948417, "f1": 59.39188237621245, "predict_samples": 5242}}, "mlqa_en_ar": {"all results": {"eval_samples": 661, "exact_match": 30.49671977507029, "f1": 43.74875680547089, "predict_samples": 6902}, "eval results": {"eval_samples": 661, "exact_match": 33.849129593810446, "f1": 46.683834716693305}, "predict results": {"exact_match": 30.49671977507029, "f1": 43.74875680547089, "predict_samples": 6902}}, "mlqa_en_de": {"all results": {"eval_samples": 657, "exact_match": 52.22492804959044, "f1": 66.28995207906392, "predict_samples": 5552}, "eval results": {"eval_samples": 657, "exact_match": 50.78125, "f1": 64.39124307492914}, "predict results": {"exact_match": 52.22492804959044, "f1": 66.28995207906392, "predict_samples": 5552}}, "mlqa_en_en": {"all results": {"eval_samples": 1440, "exact_match": 66.81622088006903, "f1": 80.1927451885261, "predict_samples": 14706}, "eval results": {"eval_samples": 1440, "exact_match": 66.2020905923345, "f1": 79.09622060488314}, "predict results": {"exact_match": 66.81622088006903, "f1": 80.1927451885261, "predict_samples": 14706}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 43.72739387016943, "f1": 64.96254036751851, "predict_samples": 5456}, "eval results": {"eval_samples": 537, "exact_match": 47.6, "f1": 67.99093097240417}, "predict results": {"exact_match": 43.72739387016943, "f1": 64.96254036751851, "predict_samples": 5456}}}}, "squad": {"layers 1": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 4.862819299905393, "f1": 10.883660980780592, "init_mem_cpu_alloc_delta": 2407129088, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 397473280, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 9547776, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 221786112, "train_runtime": 739.0634, "train_samples": 88880, "train_samples_per_second": 45.098}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 4.862819299905393, "f1": 10.883660980780592}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2407129088, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 397473280, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 9547776, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 221786112, "train_runtime": 739.0634, "train_samples": 88880, "train_samples_per_second": 45.098}}, "layers 2": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 5.212866603595081, "f1": 11.520345749079778, "init_mem_cpu_alloc_delta": 2391797760, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 425824768, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 10268672, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 1354.56, "train_samples": 88880, "train_samples_per_second": 24.606}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 5.212866603595081, "f1": 11.520345749079778}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2391797760, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 425824768, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 10268672, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 1354.56, "train_samples": 88880, "train_samples_per_second": 24.606}}, "layers 3": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 6.0927152317880795, "f1": 12.382392393896655, "init_mem_cpu_alloc_delta": 2371440640, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 454176256, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 8933376, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 2029.5349, "train_samples": 88880, "train_samples_per_second": 16.422}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 6.0927152317880795, "f1": 12.382392393896655}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2371440640, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 454176256, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 8933376, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 2029.5349, "train_samples": 88880, "train_samples_per_second": 16.422}}, "layers 4": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 9.366130558183539, "f1": 15.912273971121914, "init_mem_cpu_alloc_delta": 2325655552, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 482527744, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 10063872, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 2698.9669, "train_samples": 88880, "train_samples_per_second": 12.349}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 9.366130558183539, "f1": 15.912273971121914}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2325655552, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 482527744, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 10063872, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 2698.9669, "train_samples": 88880, "train_samples_per_second": 12.349}}, "layers 5": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 25.6764, "f1": 36.0227, "init_mem_cpu_alloc_delta": 2325655552, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 482527744, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 10063872, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 2698.9669, "train_samples": 88880, "train_samples_per_second": 12.349}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 25.6764, "f1": 36.0227}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2325655552, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 482527744, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 10063872, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 2698.9669, "train_samples": 88880, "train_samples_per_second": 12.349}}, "layers 6": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 44.333, "f1": 55.4174, "init_mem_cpu_alloc_delta": 2325655552, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 482527744, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 10063872, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 2698.9669, "train_samples": 88880, "train_samples_per_second": 12.349}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 44.333, "f1": 55.4174}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2325655552, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 482527744, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 10063872, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 2698.9669, "train_samples": 88880, "train_samples_per_second": 12.349}}, "layers 12": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 81.76915799432356, "f1": 89.03971206210255, "init_mem_cpu_alloc_delta": 1429630976, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 709339648, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 14454784, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 4050.0052, "train_samples": 88880, "train_samples_per_second": 8.23}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 81.76915799432356, "f1": 89.03971206210255}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1429630976, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 709339648, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 14454784, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 4050.0052, "train_samples": 88880, "train_samples_per_second": 8.23}}, "layers 7": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 55.07095553453169, "f1": 64.85871339317988, "init_mem_cpu_alloc_delta": 1567072256, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 567582208, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 16154624, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 2480.4302, "train_samples": 88880, "train_samples_per_second": 13.437}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 55.07095553453169, "f1": 64.85871339317988}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 1567072256, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 567582208, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 16154624, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 2480.4302, "train_samples": 88880, "train_samples_per_second": 13.437}}, "layers 8": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 61.36234626300851, "f1": 70.8619649229535, "init_mem_cpu_alloc_delta": 2206949376, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 595933696, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 9375744, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 4937.895, "train_samples": 88880, "train_samples_per_second": 6.75}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 61.36234626300851, "f1": 70.8619649229535}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2206949376, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 595933696, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 9375744, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 4937.895, "train_samples": 88880, "train_samples_per_second": 6.75}}, "layers 9": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 76.74550614947965, "f1": 85.62834715156818, "init_mem_cpu_alloc_delta": 2186743808, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 624285184, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 12419072, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 5574.0873, "train_samples": 88880, "train_samples_per_second": 5.979}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 76.74550614947965, "f1": 85.62834715156818}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2186743808, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 624285184, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 12419072, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 5574.0873, "train_samples": 88880, "train_samples_per_second": 5.979}}, "layers 10": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 80.65279091769158, "f1": 88.56491861246218, "init_mem_cpu_alloc_delta": 2137829376, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 652636672, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 10035200, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 6147.0948, "train_samples": 88880, "train_samples_per_second": 5.422}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 80.65279091769158, "f1": 88.56491861246218}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2137829376, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 652636672, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 10035200, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 6147.0948, "train_samples": 88880, "train_samples_per_second": 5.422}}, "layers 11": {"all results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 81.5042573320719, "f1": 88.89397783802431, "init_mem_cpu_alloc_delta": 2152390656, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 680988160, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 10878976, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 6827.3474, "train_samples": 88880, "train_samples_per_second": 4.882}, "eval results": {"epoch": 3.0, "eval_samples": 10851, "exact_match": 81.5042573320719, "f1": 88.89397783802431}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 2152390656, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 680988160, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 10878976, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 95232, "train_mem_gpu_peaked_delta": 231223296, "train_runtime": 6827.3474, "train_samples": 88880, "train_samples_per_second": 4.882}}}}