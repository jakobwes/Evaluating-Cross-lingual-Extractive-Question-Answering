{"mlqa": {"mlqa_en_en": {"all results": {"eval_samples": 1476, "exact_match": 67.67903364969801, "f1": 80.8409433117442, "predict_samples": 15269}, "eval results": {"eval_samples": 1476, "exact_match": 68.6411149825784, "f1": 80.89696090183098}, "predict results": {"exact_match": 67.67903364969801, "f1": 80.8409433117442, "predict_samples": 15269}}, "mlqa_ar_en": {"all results": {"eval_samples": 674, "exact_match": 29.878163074039364, "f1": 46.07120428727677, "predict_samples": 6793}, "eval results": {"eval_samples": 674, "exact_match": 29.013539651837526, "f1": 44.45085829877253}, "predict results": {"exact_match": 29.878163074039364, "f1": 46.07120428727677, "predict_samples": 6793}}, "mlqa_en_ar": {"all results": {"eval_samples": 671, "exact_match": 12.03373945641987, "f1": 19.006080096198083, "predict_samples": 7119}, "eval results": {"eval_samples": 671, "exact_match": 12.18568665377176, "f1": 19.704454408165095}, "predict results": {"exact_match": 12.03373945641987, "f1": 19.006080096198083, "predict_samples": 7119}}, "mlqa_ar_ar": {"all results": {"eval_samples": 678, "exact_match": 34.058106841611995, "f1": 52.43040751752009, "predict_samples": 6810}, "eval results": {"eval_samples": 678, "exact_match": 33.07543520309478, "f1": 50.90631494042677}, "predict results": {"exact_match": 34.058106841611995, "f1": 52.43040751752009, "predict_samples": 6810}}, "mlqa_es_es": {"all results": {"eval_samples": 537, "exact_match": 46.10698648391396, "f1": 66.92431675712389, "predict_samples": 5457}, "eval results": {"eval_samples": 537, "exact_match": 49.0, "f1": 69.26341139796078}, "predict results": {"exact_match": 46.10698648391396, "f1": 66.92431675712389, "predict_samples": 5457}}, "mlqa_en_de": {"all results": {"eval_samples": 673, "exact_match": 41.930484835067524, "f1": 54.46365463245687, "predict_samples": 5759}, "eval results": {"eval_samples": 673, "exact_match": 44.7265625, "f1": 56.22311579921519}, "predict results": {"exact_match": 41.930484835067524, "f1": 54.46365463245687, "predict_samples": 5759}}, "mlqa_de_de": {"all results": {"eval_samples": 593, "exact_match": 46.26964799645783, "f1": 61.733880702703146, "predict_samples": 5278}, "eval results": {"eval_samples": 593, "exact_match": 46.2890625, "f1": 62.09726687409957}, "predict results": {"exact_match": 46.26964799645783, "f1": 61.733880702703146, "predict_samples": 5278}}}, "squad": {"all results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 82.06244087038789, "f1": 89.09539709124654, "init_mem_cpu_alloc_delta": 991453184, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 1109893120, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 14753792, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 3330195456, "train_mem_gpu_peaked_delta": 8287144960, "train_runtime": 11376.3034, "train_samples": 89597, "train_samples_per_second": 1.477}, "eval results": {"epoch": 3.0, "eval_samples": 10918, "exact_match": 82.06244087038789, "f1": 89.09539709124654}, "train results": {"epoch": 3.0, "init_mem_cpu_alloc_delta": 991453184, "init_mem_cpu_peaked_delta": 0, "init_mem_gpu_alloc_delta": 1109893120, "init_mem_gpu_peaked_delta": 0, "train_mem_cpu_alloc_delta": 14753792, "train_mem_cpu_peaked_delta": 0, "train_mem_gpu_alloc_delta": 3330195456, "train_mem_gpu_peaked_delta": 8287144960, "train_runtime": 11376.3034, "train_samples": 89597, "train_samples_per_second": 1.477}}}